{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "de81d4fc-e960-42f0-886b-83603443b641",
       "rows": [
        [
         "0",
         "2075884494",
         "First year Wilkinson Microwave Anisotropy Probe (WMAP) observations: Preliminary maps and basic results",
         "We present full-sky microwave maps in five frequency bands (23-94 GHz) from the Wilkinson Microwave Anisotropy Probe (WMAP) first-year sky survey. Calibration errors are less than 0.5%, and the low systematic error level is well specified. The cosmic microwave background (CMB) is separated from the foregrounds using multifrequency data. The sky maps are consistent with the 7° FWHM Cosmic Background Explorer (COBE) maps. We report more precise, but consistent, dipole and quadrupole values. The CMB anisotropy obeys Gaussian statistics with -58 < fNL < 134 (95% confidence level [CL]). The 2 ≤ l ≤ 900 anisotropy power spectrum is cosmic-variance-limited for l < 354, with a signal-to-noise ratio greater than 1 per mode to l = 658. The temperature-polarization cross-power spectrum reveals both acoustic features and a large-angle correlation from reionization. The optical depth of reionization is τ = 0.17 ± 0.04, which implies a reionization epoch of tr = 180 Myr (95% CL) after the big bang at a redshift of zr = 20 (95% CL) for a range of ionization scenarios. This early reionization is incompatible with the presence of a significant warm dark matter density.     A best-fit cosmological model to the CMB and other measures of large-scale structure works remarkably well with only a few parameters. The age of the best-fit universe is t0 = 13.7 ± 0.2 Gyr. Decoupling was tdec = 379 kyr after the big bang at a redshift of zdec = 1089 ± 1. The thickness of the decoupling surface was Δzdec = 195 ± 2. The matter density of the universe is Ωmh2 = 0.135, the baryon density is Ωbh2 = 0.0224 ± 0.0009, and the total mass-energy of the universe is Ωtot = 1.02 ± 0.02. It appears that there may be progressively less fluctuation power on smaller scales, from WMAP to fine-scale CMB measurements to galaxies and finally to the Lyα forest. This may be accounted for with a running spectral index of scalar fluctuations, fitted as ns = 0.93 ± 0.03 at wavenumber k0 = 0.05 Mpc-1 (leff ≈ 700), with a slope of dns/d ln k = -0.031 in the best-fit model. (For WMAP data alone, ns = 0.99 ± 0.04.) This flat universe model is composed of 4.4% baryons, 22% dark matter, and 73% dark energy. The dark energy equation of state is limited to w < -0.78 (95% CL). Inflation theory is supported with ns ≈ 1, Ωtot ≈ 1, Gaussian random phases of the CMB anisotropy, and superhorizon fluctuations implied by the temperature-polarization anticorrelations at decoupling. An admixture of isocurvature modes does not improve the fit. The tensor-to-scalar ratio is r(k0 = 0.002 Mpc-1) < 0.90 (95% CL). The lack of CMB fluctuation power on the largest angular scales reported by COBE and confirmed by WMAP is intriguing. WMAP continues to operate, so results will improve."
        ],
        [
         "1",
         "2100507804",
         "The Anti-k(t) jet clustering algorithm",
         "The kt and Cambridge/Aachen inclusive jet finding algorithms for hadron-hadron collisions can be seen as belonging to a broader class of sequential recombination jet algorithms, parametrised by the power of the energy scale in the distance measure. We examine some properties of a new member of this class, for which the power is negative. This ``anti-kt'' algorithm essentially behaves like an idealised cone algorithm, in that jets with only soft fragmentation are conical, active and passive areas are equal, the area anomalous dimensions are zero, the non-global logarithms are those of a rigid boundary and the Milan factor is universal. None of these properties hold for existing sequential recombination algorithms, nor for cone algorithms with split-merge steps, such as SISCone. They are however the identifying characteristics of the collinear unsafe plain ``iterative cone'' algorithm, for which the anti-kt algorithm provides a natural, fast, infrared and collinear safe replacement."
        ],
        [
         "2",
         "2107816296",
         "The Catchment Area of Jets",
         "The area of a jet is a measure of its susceptibility to radiation, like pileup or underlying event (UE), that on average, in the jet’s neighbourhood, is uniform in rapidity and azimuth. In this article we establish a theoretical grounding for the discussion of jet areas, introducing two main definitions, passive and active areas, which respectively characterise the sensitivity to pointlike or diffuse pileup and UE radiation. We investigate the properties of jet areas for three standard jet algorithms, kt, Cambridge/Aachen and SISCone. Passive areas for single-particle jets are equal to the naive geometrical expectation πR 2 , but acquire an anomalous dimension at higher orders in the coupling, calculated here at leading order. The more physically relevant active areas differ from πR 2 even for single-particle jets, substantially so in the case of the cone algorithms like SISCone with a Tevatron Run-II split–merge procedure. We compare our results with direct measures of areas in parton-shower Monte Carlo simulations and find good agreement with the main features of the analytical predictions. We furthermore justify the use of jet areas to subtract the contamination from pileup."
        ],
        [
         "3",
         "1639032689",
         "Genetic Algorithms in Search, Optimization, and Machine Learning",
         "From the Publisher:\r\nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \r\n\r\nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
        ],
        [
         "4",
         "1999864907",
         "Higgs look-alikes at the LHC",
         "The discovery of a Higgs particle is possible in a variety of search channels at the LHC. However, the true identity of any putative Higgs boson will, at first, remain ambiguous until one has experimentally excluded other possible assignments of quantum numbers and couplings. We quantify the degree to which one can discriminate a standard model Higgs boson from “look-alikes” at, or close to, the moment of discovery at the LHC. We focus on the fully-reconstructible golden decay mode to a pair of Z bosons and a four-lepton final state. Considering both on-shell and off-shell Z’s, we show how to utilize the full decay information from the events, including the distributions and correlations of the five relevant angular variables. We demonstrate how the finite phase space acceptance of any LHC detector sculpts the decay distributions, a feature neglected in previous studies. We use likelihood ratios to discriminate a standard model Higgs from look-alikes with other spins or nonstandard parity, CP, or form factors. For a resonance mass of 200  GeV/c^2, we achieve a median discrimination significance of 3σ with as few as 19 events, and even better discrimination for the off-shell decays of a 145  GeV/c^2 resonance."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2075884494</td>\n",
       "      <td>First year Wilkinson Microwave Anisotropy Prob...</td>\n",
       "      <td>We present full-sky microwave maps in five fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2100507804</td>\n",
       "      <td>The Anti-k(t) jet clustering algorithm</td>\n",
       "      <td>The kt and Cambridge/Aachen inclusive jet find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2107816296</td>\n",
       "      <td>The Catchment Area of Jets</td>\n",
       "      <td>The area of a jet is a measure of its suscepti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1639032689</td>\n",
       "      <td>Genetic Algorithms in Search, Optimization, an...</td>\n",
       "      <td>From the Publisher:\\r\\nThis book brings togeth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999864907</td>\n",
       "      <td>Higgs look-alikes at the LHC</td>\n",
       "      <td>The discovery of a Higgs particle is possible ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  2075884494  First year Wilkinson Microwave Anisotropy Prob...   \n",
       "1  2100507804             The Anti-k(t) jet clustering algorithm   \n",
       "2  2107816296                         The Catchment Area of Jets   \n",
       "3  1639032689  Genetic Algorithms in Search, Optimization, an...   \n",
       "4  1999864907                       Higgs look-alikes at the LHC   \n",
       "\n",
       "                                            abstract  \n",
       "0  We present full-sky microwave maps in five fre...  \n",
       "1  The kt and Cambridge/Aachen inclusive jet find...  \n",
       "2  The area of a jet is a measure of its suscepti...  \n",
       "3  From the Publisher:\\r\\nThis book brings togeth...  \n",
       "4  The discovery of a Higgs particle is possible ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd  \n",
    "\n",
    "DOC_PATH = '../data/magsample/documents.json'\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(DOC_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        data.append((str(doc['id']), doc['title'], doc['abstract']))\n",
    "\n",
    "df_docs = pd.DataFrame(data, \n",
    "                       columns=['id', 'title', 'abstract'])\n",
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the duplicated docs and generating a map from title -> doc_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_doc_ids = df_docs.groupby('title').id.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `duplicated_doc_ids` can be seen as a ground truth, since we'll use title as queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ok, so we have some duplicated docs. The problem with that is when we do negative sampling, we can sample the same doc as a negative example for a positive example. This is not good. \n",
    "\n",
    "To handle that, we will index the docs abstracts. Then, when we return the top-k docs by its title, we remove possible duplicate ids from the result.\n",
    "\n",
    "Steps:\n",
    "1. Index the documents: abstract -> id; \n",
    "2. The query will be the title;\n",
    "3. Return the top-k docs by the title;\n",
    "4. If the doc id associated with title has duplicate, remove the duplicates from the result.\n",
    "5. If the doc id associated with title is not in the results, put it in the results at the first position with 2x the score of the second result.\n",
    "6. If the doc id associated with title is in the results but not in the first position, put it in the first position with 2x the score of the second result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arch.png               \u001b[0m\u001b[01;34mfull_scripts\u001b[0m/  requirements.txt\n",
      "\u001b[01;34mcustom_scripts\u001b[0m/        keep.txt       \u001b[01;34mt5_decoder_start_token_embeds\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/                  \u001b[01;34mnotebook\u001b[0m/      \u001b[01;34mt5_pretrainer\u001b[0m/\n",
      "\u001b[01;34mfull_16_1024_scripts\u001b[0m/  README.md      \u001b[01;34mvenv\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elves/WorkSpaces/RIPOR/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Mar 13, 2025 10:15:17 AM org.apache.lucene.store.MMapDirectory lookupProvider\n",
      "WARNING: You are running with Java 20 or later. To make full use of MMapDirectory, please update Apache Lucene.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2025-03-13 10:15:17,243 INFO  [main] index.SimpleIndexer (SimpleIndexer.java:141) - Using DefaultEnglishAnalyzer\n",
      "2025-03-13 10:15:17,245 INFO  [main] index.SimpleIndexer (SimpleIndexer.java:142) - Stemmer: porter\n",
      "2025-03-13 10:15:17,245 INFO  [main] index.SimpleIndexer (SimpleIndexer.java:143) - Keep stopwords? false\n",
      "2025-03-13 10:15:17,245 INFO  [main] index.SimpleIndexer (SimpleIndexer.java:144) - Stopwords file: null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127716it [00:38, 3288.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyserini.index.lucene import LuceneIndexer\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "INDEX_PATH = '../indices/magsample'\n",
    "\n",
    "indexer = LuceneIndexer(INDEX_PATH)\n",
    "\n",
    "qrels: Dict[str, str] = {}\n",
    "qid_to_query: Dict[str, str] = {}\n",
    "\n",
    "data = []\n",
    "for i, row in tqdm(df_docs.iterrows()):\n",
    "    doc_id = row['id']\n",
    "    \n",
    "    doc_title = re.sub('\\s+', ' ', row['title']).strip()\n",
    "    doc_abstract = re.sub('\\s+', ' ', row['abstract']).strip()\n",
    "    \n",
    "    qrels[str(i)] = doc_id\n",
    "\n",
    "    doc = {\n",
    "        'id': doc_id,\n",
    "        'contents': doc_abstract\n",
    "    }\n",
    "\n",
    "    indexer.add_doc_dict(doc)\n",
    "    qid_to_query[str(i)] = doc_title \n",
    "\n",
    "    data.append((doc_id, doc_abstract))\n",
    "\n",
    "indexer.close()\n",
    "\n",
    "os.makedirs('../data/magsample/collection/', exist_ok=True)\n",
    "df_data = pd.DataFrame(data, columns=['id', 'abstract'])\n",
    "df_data.to_csv('../data/magsample/collection/raw.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Let's check if all docs have been indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "all_qids = list(qid_to_query.keys())\n",
    "\n",
    "dev_qids = set(random.sample(all_qids, int(0.1 * len(all_qids)))) \n",
    "train_qids = set(all_qids) - dev_qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127716, 12771, 114945)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qids), len(dev_qids), len(train_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.makedirs('../data/magsample/train_queries/', exist_ok=True)\n",
    "os.makedirs('../data/magsample/dev_queries/', exist_ok=True)\n",
    "\n",
    "train_filepath = '../data/magsample/train_queries/raw.tsv'\n",
    "dev_filepath = '../data/magsample/dev_queries/raw.tsv'\n",
    "\n",
    "with open(train_filepath, 'w') as w_train, open(dev_filepath, 'w') as w_dev:\n",
    "    for qid, query in qid_to_query.items():\n",
    "        if qid in train_qids:\n",
    "            w_train.write(f'{qid}\\t{query}\\n')\n",
    "        else:\n",
    "            w_dev.write(f'{qid}\\t{query}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qid_to_reldocs = {qid: [qrels[qid]] for qid in train_qids}\n",
    "dev_qid_to_reldocs = {qid: [qrels[qid]] for qid in dev_qids}\n",
    "\n",
    "os.makedirs('../data/magsample/train_qrels/', exist_ok=True)\n",
    "os.makedirs('../data/magsample/dev_qrels/', exist_ok=True)\n",
    "\n",
    "train_qrels_filepath = '../data/magsample/train_qrels/qid_to_reldocids.json'\n",
    "dev_qrels_filepath = '../data/magsample/dev_qrels/qid_to_reldocids.json'\n",
    "\n",
    "with open(train_qrels_filepath, 'w') as f:\n",
    "    f.write(json.dumps(train_qid_to_reldocs))\n",
    "\n",
    "with open(dev_qrels_filepath, 'w') as f:\n",
    "    f.write(json.dumps(dev_qid_to_reldocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "qrel_path = '../data/magsample/dev_qrels/'\n",
    "with open(qrel_path + 'qid_to_reldocids.json') as reader, open(qrel_path + 'qrel.json', 'w') as writer:\n",
    "    dev_qid_to_reldocs = json.load(reader)\n",
    "\n",
    "    qrel = {}\n",
    "    for qid, reldocs in dev_qid_to_reldocs.items():\n",
    "        qrel[qid] = {docid: 1 for docid in reldocs}\n",
    "\n",
    "    writer.write(json.dumps(qrel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14845/114945 [01:30<10:04, 165.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Yicesä2.2\" - No hits found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114945/114945 [12:00<00:00, 159.63it/s]\n"
     ]
    }
   ],
   "source": [
    "searcher = LuceneSearcher(INDEX_PATH)\n",
    "k = 100\n",
    "\n",
    "os.makedirs('../data/magsample/bm25_run/', exist_ok=True)\n",
    "\n",
    "with open('../data/magsample/bm25_run/qrel_added_qid_docids_teacher_scores.train.jsonl', 'w') as f:\n",
    "    for qid in tqdm(train_qids):\n",
    "        rel_doc = qrels[qid]\n",
    "        \n",
    "        query = qid_to_query[qid]\n",
    "\n",
    "        hits = searcher.search(query, k=k)\n",
    "\n",
    "        if len(hits) == 0:\n",
    "            print(f'\"{query}\" - No hits found')\n",
    "            continue\n",
    "\n",
    "        docids = []\n",
    "        scores = []\n",
    "\n",
    "        # if the ground truth isn't in the hits, we put it there in the first position\n",
    "        if doc_id not in [hit.docid for hit in hits]:\n",
    "            # the score will be 1.5x the score of the first hit\n",
    "            score = hits[0].score * 1.25\n",
    "\n",
    "            docids.append(doc_id)\n",
    "            scores.append(round(score, 5))\n",
    "\n",
    "        # if the ground truth isn't in the first position, we put it there\n",
    "        elif hits[0].docid != doc_id:\n",
    "            # the the score of ground truth\n",
    "            score = hits[0].score * 1.25\n",
    "            for hit in hits:\n",
    "                if hit.docid == doc_id:\n",
    "                    score = hit.score\n",
    "                    break\n",
    "            \n",
    "            # the score of the ground truth will be the score of the first hit + the difference between the scores\n",
    "            score = hits[0].score + (hits[0].score - score)\n",
    "\n",
    "            # remove the ground truth from the hits\n",
    "            hits = [hit for hit in hits if hit.docid != doc_id]\n",
    "\n",
    "            docids.append(doc_id)\n",
    "            scores.append(round(score, 5))\n",
    "\n",
    "        # if the query has duplicates, we remove the duplicates from the hits\n",
    "        has_duplicates = len(duplicated_doc_ids[query]) > 1\n",
    "        if has_duplicates:\n",
    "            doc_ids_to_remove = duplicated_doc_ids[query].copy()\n",
    "            doc_ids_to_remove.remove(rel_doc)\n",
    "            hits = [hit for hit in hits if hit.docid not in doc_ids_to_remove]\n",
    "\n",
    "        # Now we add the hits to the results\n",
    "        for hit in hits:\n",
    "            docids.append(hit.docid)\n",
    "            scores.append(round(hit.score, 5))\n",
    "\n",
    "        if docids[0] != doc_id:\n",
    "            raise Exception(f'\"{query}\" - Ground truth not in first position: {doc_id} - {docids}')\n",
    "\n",
    "        f.write(json.dumps({\n",
    "            'qid': qid,\n",
    "            'docids': docids,\n",
    "            'scores': scores\n",
    "        }) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
